<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jkschin.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jkschin.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-02-03T23:21:24-05:00</updated><id>https://jkschin.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">PhD Reflection (and hopefully advice to prospective students)</title><link href="https://jkschin.com/blog/2024/phd-reflection/" rel="alternate" type="text/html" title="PhD Reflection (and hopefully advice to prospective students)" /><published>2024-01-29T10:59:00-05:00</published><updated>2024-01-29T10:59:00-05:00</updated><id>https://jkschin.com/blog/2024/phd-reflection</id><content type="html" xml:base="https://jkschin.com/blog/2024/phd-reflection/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>It’s the final week of the Independent Activities Period (IAP) at MIT and I attended an event titled “IAP 2024: Expanding Horizons in Computing”.
I consciously chose not to link this page as it’s definitely going to become stale one day and I do not want to have to deal with that.
I have other reflections on the talks, but I want to highlight the importance of stepping back from research once in awhile to see what’s out there.
In particular, I chose to attend the talks on Computer Security.
I learnt about Fully Homomorphic Encryption, amongst other things.
However, my biggest takeaway was searching for a speaker (Henry Corrigan-Gibbs) and coming across his <a href="https://people.csail.mit.edu/henrycg/">personal page</a>.
Under the advice section on CG’s page, I saw three articles from
<a href="https://www.cs.cmu.edu/~mblum/research/pdf/grad.html">Manuel Blum</a>,
<a href="https://www.cs.cmu.edu/~harchol/gradschooltalk.pdf">Mor Harchol-Balter</a> and
<a href="https://www.cs.virginia.edu/~robins/YouAndYourResearch.html">Richard Hamming</a> and read them (technically the one from Hamming is a transcribed lecture).
I started by browsing the article from Manuel Blum and he mentioned that you should <em>always</em> be writing.
I immediately took out a pen and paper and started to write some thoughts and then decided to write a proper reflection as I’m currently midway (approximately) through my PhD (I’m all done with classes and finished my Thesis Proposal meeting).
This reflection serves to synthesize my thoughts and hopefully inform prospective students on whether or not they should pursue a PhD.
In addition, I write some thoughts on how to approach research.
Nothing beats reading those three articles yourself though!</p>

<h2 id="why-do-a-phd">Why do a PhD?</h2>

<p>To answer that question, I’d have to start with a brief history of my life trajectory.
I graduated in 2015 with a degree in computer science.
Unsure of what to do next, I joined a government research lab and did applied research.
I was interested in the research problems and started reading many papers from Computer Vision (CV) conferences like CVPR, ICCV and ECCV.
This was also the start of the deep learning revolution and AlexNet was only published three years ago.
I still recall using Caffe and some other weird deep learning frameworks - those were really hard to use.
Seeing an opportunity to commercialize CV, I joined an accelerator to start a CV company.
That didn’t work out and I joined Palantir in 2017.
Even at Palantir as a software engineer, I dug into the code, tried to fully understand the frameworks, the architecture decisions, etc.
I read many textbooks on the many flights that I was on.
I found myself attracted to hard problems and diving deep into something and fully understanding it.
Mor Harchol-Balter’s comment resonates with me:</p>

<blockquote>
  <p><strong>There’s also the joy of doing it right</strong>. In a company, the aim is to
get a working product and ship it out quickly. In research, you can take your time
and plan out your project so that you are proud to defend every one of your design
decisions. Research is not about simple heuristics or quick hacks. Many people
also relish the joy of being the authority on an area and of having their work read
and cited by others.
– <cite>Mor Harchol-Balter</cite></p>
</blockquote>

<p>Indeed, there is joy in itself of having the freedom to plan something, execute it, and doing it right within the set of resource constrants you’re given.
I say this, as even within a PhD, you do not have infinite time.
You have more time to dive deeper into something when compared to a commercial environment, but time and money is still finite.
I guess while at Palantir, I wanted to do a PhD at the back of my mind and do more work in deep learning (commonly called AI nowadays but I’m still old school).
Well, I’m at MIT for a PhD now, and the rest is history.
I’m right about midway through my PhD and I have finished all my required classes.
I do enjoy taking classes, and would continue taking perhaps one class each semester.
However, it would probably not be optimal to do that, and Mor Harchol-Balter discusses it more in her document.
Coming from industry, I thought how she described a PhD was largely accurate:</p>

<blockquote>
  <p>The Ph.D. is a tremendous opportunity. You get to pick an advisor in any research area you like and then you get to do research in that area, receive mentoring,
<strong>think deeply on problems</strong>, publish papers, become famous, while paying zero tuition for 6 years and receiving a salary.
– <cite>Mor Harchol-Balter</cite></p>
</blockquote>

<p>So what’s next after my PhD?
I haven’t thought it out thoroughly, and this is bound to change.
I know I do want to build a product or commercialize my research and start a company.
I am also keen on research and AI is really an exciting field now.
In particular, the recent papers on <a href="https://deepmind.google/discover/blog/millions-of-new-materials-discovered-with-deep-learning/">discovering new materials from DeepMind</a> is really cool.
Having a PhD gives me the option of doing research upon graduation.</p>

<blockquote>
  <p>When making a decision about the next 6 years of your life, it’s good to stop and
think about what you might do when you finish. Most students upon completing a
Ph.D. either go into academia (research university or teaching school) and become
a professor, or they go to a research lab. Some people never do research again after
completing a Ph.D. For such people, the Ph.D. was largely a waste of time.
– <cite>Mor Harchol-Balter</cite></p>
</blockquote>

<p>If I don’t do research after graduation, would it really largely be a waste of time?
It was a great intellectual exercise and I got to dive deeper into some of the topics that I am interested in.
Put another way, this was probably my only chance of doing it.
I don’t think I would do it at an older age as I would have other priorities then.</p>

<h2 id="how-should-you-approach-research">How should you approach research?</h2>

<p>I read The Power of Habit by Charles Duhigg last year.
A key insight I got was that if you want to shape your mind to do X instead of Y (where X is the optimal thing to do) when a certain cue happens, you can write down exactly what you will do when you face that cue and it will be much easier to tune your mind to the good habit.
Why does this matter?
Well, a lot of times in your PhD, you will be tempted to just read for the sake of reading, with no clear goal in sight.
Or when you are stuck on a problem for 1 hour and you finally have the Eureka moment - you decide to take a break and come back to it 15 minutes later, only to spend another 30 minutes trying to figure out how you got to the Eureka moment.
The optimal thing to do there is simply to just sit for another 5 to 10 minutes, write down your thoughts, and then take a break.
If all this sounds familiar to you, then the next few sections will be extremely relevant.
The next few sections will be structured around cues and how to handle them - practical tips that you can directly execute.</p>

<p><strong>It’s the start of a new semester. You’re in this exciting new class. You look at all the readings and the textbooks, and you decide to read textbooks end to end.</strong></p>

<p>I love reading.
In particular, I love reading a textbook end-to-end.
With infinite time, it’s possibly effective, as you can read it over and over again and you glean new insight each time you read.
However, I quickly found that sometimes it’s important to just zoom into a concept, write some math and code, and understand that concept really well.
I was reminded of this and Manuel Blum put it really well:</p>

<blockquote>
  <p>Books are not scrolls.
Scrolls must be read like the Torah from one end to the other.
Books are random access – a great innovation over scrolls.
Make use of this innovation! Do NOT feel obliged to read a book from beginning to end.
Permit yourself to open a book and start reading from anywhere.
In the case of mathematics or physics or anything especially hard, try to find something anything that you can understand.
Read what you can.
Write in the margins. (You know how useful that can be.)
Next time you come back to that book, you’ll be able to read more.
You can gradually learn extraordinarily hard things this way.
– <cite>Manuel Blum</cite></p>
</blockquote>

<p>Clearly, the inference one can make about this is to read papers from one end to the other as it is a scroll.
How should one read a paper then?
For that, I refer the reader to <a href="https://www.youtube.com/watch?v=733m6qBH-jI">a video from Andrew Ng</a>.
It is also important to note here, some wise advice from Richard Hamming:</p>

<blockquote>
  <p>There was a fellow at Bell Labs, a very, very, smart guy. He was always in the library; he read everything. If you wanted references, you went to him and he gave you all kinds of references. But in the middle of forming these theories, I formed a proposition: there would be no effect named after him in the long run. He is now retired from Bell Labs and is an Adjunct Professor. He was very valuable; I’m not questioning that. He wrote some very good Physical Review articles; but there’s no effect named after him because he read too much. If you read all the time what other people have done you will think the way they thought. If you want to think new thoughts that are different, then do what a lot of creative people do - get the problem reasonably clear and then refuse to look at any answers until you’ve thought the problem through carefully how you would do it, how you could slightly change the problem to be the correct one. So yes, you need to keep up. You need to keep up more to find out what the problems are than to read to find the solutions. The reading is necessary to know what is going on and what is possible. But reading to get the solutions does not seem to be the way to do great research. So I’ll give you two answers. <strong>You read; but it is not the amount, it is the way you read that counts.</strong>
– <cite>Richard Hamming</cite></p>
</blockquote>

<p><strong>You now know the techniques of reading. You buy the physical textbook or you download the PDF onto your laptop or you are reading a paper. You wake up early in the morning and go to a cafe. You first start scrolling social media, and then open up your textbook or laptop. You resist reading the introduction of the textbook and focus on key chapters, or you read the paper optimally. You read a few lines, and sip a coffee, and this repeats. Half an hour later, you have the ideas all in your head. Three days later, you vaguely recall the ideas but can’t fully communicate what the concept is.</strong></p>

<p>This is a situation that happens to me often.
I know the ideas - it’s in my head.
However, it’s so hard to write it down in text.
I find myself referring back to the original paper when writing it down.
Over the years of my PhD, I found that taking out a writing pad to sketch out some ideas helps.
In fact, directly synthesizing your thoughts and writing a short literature review of it or a short blog post about it would help greatly when you’re assembling the final paper.
This can significantly improve your efficiency.
Do you have to write this for every single paper you browse or read?
Probably not.
If there’s a good chance you’re going to cite it or use the concepts in your work, then it could be worth writing a short document on it.
Again, the wisdom of Manuel Blum shines here:</p>

<blockquote>
  <p>Consider writing what you read as you read it.
This is especially true if you’re intent on reading something hard.
– <cite>Manuel Blum</cite></p>
</blockquote>

<p>and here:</p>

<blockquote>
  <p>You are all computer scientists.
You know what FINITE AUTOMATA can do.
You know what TURING MACHINES can do.
For example, Finite Automata can add but not multiply.
Turing Machines can compute any computable function.
Turing machines are incredibly more powerful than Finite Automata.
Yet the only difference between a FA and a TM is that
the TM, unlike the FA, has paper and pencil.
Think about it.
It tells you something about the power of writing.
Without writing, you are reduced to a finite automaton.
With writing you have the extraordinary power of a Turing machine.
– <cite>Manuel Blum</cite></p>
</blockquote>

<p><strong>We now have a framework of reading and writing down our ideas. You’ve also probably heard (I don’t know who first said this) of how choosing the right problem gets you 50% (or some X%) of the way there. What kind of problems should you work on?</strong></p>

<p>Regardless of whether or not you intend to go into academia or industry, you want to do impressive work.
After all, what you write and publish will be on the internet <em>forever</em>.
You would want it to be something you’re proud of - right?
Of course, you can choose the hardest problems in the world to solve, like teleportation or time travel (see below), but you’re never going to get a PhD from that (at least as of writing, I don’t see how this is remotely possible).
Richard Hamming succinctly puts it:</p>

<blockquote>
  <p>If you do not work on an important problem, it’s unlikely you’ll do important work. It’s perfectly obvious. Great scientists have thought through, in a careful way, a number of important problems in their field, and they keep an eye on wondering how to attack them. Let me warn you, `important problem’ must be phrased carefully. The three outstanding problems in physics, in a certain sense, were never worked on while I was at Bell Labs. By important I mean guaranteed a Nobel Prize and any sum of money you want to mention. We didn’t work on (1) time travel, (2) teleportation, and (3) antigravity. They are not important problems because we do not have an attack. It’s not the consequence that makes a problem important, it is that you have a reasonable attack. That is what makes a problem important.
– <cite>Richard Hamming</cite></p>
</blockquote>

<p><strong>You now have decided on a problem, and also a reasonable attack. But what does a reasonable attack mean? What if you get stuck?</strong></p>

<p>After reading tons of papers, you get a sense of the field and see some gaps that you can plug.
You have it in your head, or perhaps written down, a strategy to attack the problem.
Inevitably, you will face bumps along the way and get stuck on a step in your strategy.
This could be anything from making a wrong assumption about something, acquiring new information and realizing you were wrong, and so on.
Whenever I get stuck, I find myself going outside for a walk and thinking in my head.
I get myself unstuck and I end up not writing down the steps to get unstuck at times, and I have to repeat this process.
This goes back to the earlier point of Manuel Blum earlier and writing - always be writing.
In fact, I would say that <em>writing</em> and <em>typing</em> is fundamentally different.
I can type faster than I write (most do), and I can easily do a brain dump of whatever is in my head.
However, the benefit of writing is that you <em>cannot</em> write as fast - your brain is forced to be efficient and synthesizes the most important things, which in itself could be valuable.
The importance of writing might be the <strong>side benefit of synthesis</strong>.
I found that when I do a typing brain dump, I have to revisit it and synthesize it.
More importantly, the reason why I am typing all this, is that I am doing some form of meta thinking about my processes of typing and writing - I am thinking about how I think.
When you are stuck, think about how you’re thinking.
Why are you thinking this way?
Do other people think this way?
Is there someone else that thinks differently and you can learn from her?
As Manuel Blum puts it:</p>

<blockquote>
  <p>There will come a time when you work on a problem long and hard but UNsuccessfully :(
And then you learn that someone else found a solution.
See this as the GREAT opportunity it is to learn something important.
Don’t let it pass you by.
Ask yourself: “How SHOULD I have been thinking to solve that problem?”
I have found that doing so is a powerful exercise.
Danny Sleator tells me that BOB FLOYD independently recommended exactly this exercise to his students.
He would lead them into asking themselves:
“How COULD I have led myself to that answer?”
Take the time to think it through.
It’s worth it.
– <cite>Manuel Blum</cite></p>
</blockquote>

<p>PhD students are obsessed with thinking about the problem and finding solutions.
However, very few spend time thinking about how they are thinking, and this is quite important.</p>

<h2 id="conclusion">Conclusion</h2>

<p>I have more to discuss, but I think it is a good time to take a pause here.
We discussed why you should do a PhD and if you have decided on doing one, how you should approach research in terms of reading, finding an important problem and also getting yourself unstuck.
Nothing beats reading the articles themselves, and I probably will re-read them at some point.
My PhD involves writing quite a bit of code, and I do want to write an actual tactical plan on how to execute a research paper, but I will leave this for another post day and keep this focused.</p>]]></content><author><name></name></author><category term="personal" /><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Intricacies of nn.CrossEntropyLoss Ignore Index and Gradients</title><link href="https://jkschin.com/blog/2023/cross-entropy-loss-ignore-index/" rel="alternate" type="text/html" title="Intricacies of nn.CrossEntropyLoss Ignore Index and Gradients" /><published>2023-05-25T11:59:00-04:00</published><updated>2023-05-25T11:59:00-04:00</updated><id>https://jkschin.com/blog/2023/cross-entropy-loss-ignore-index</id><content type="html" xml:base="https://jkschin.com/blog/2023/cross-entropy-loss-ignore-index/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In machine learning, PyTorch’s <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code> is an often-utilized function.
It combines <code class="language-plaintext highlighter-rouge">nn.LogSoftmax()</code> and <code class="language-plaintext highlighter-rouge">nn.NLLLoss()</code> in one single class to compute the cross-entropy loss between the predicted and actual labels.
The CrossEntropyLoss function offers an optional parameter called <code class="language-plaintext highlighter-rouge">ignore_index</code> that can be used to ignore the loss contribution from some specific classes and is widely used in NLP, where padding of a sequence is necessary.
This post delves into the intricacies of this mechanism and how it affects gradients in backpropagation.
I was using <code class="language-plaintext highlighter-rouge">ignore_index</code> and ran into an issue with <code class="language-plaintext highlighter-rouge">nan</code> values after the first optimization step and had to dive deeper into the issue.
As with all code debugging nowadays (and writing), I used ChatGPT to assist me in writing this post.</p>

<h2 id="context">Context</h2>

<p>I was training a GPT based model and the final output is postprocessed with a mask.
In a classic GPT language model, you simply softmax over the vocabulary and it outputs the relevant token.
However, I wanted to mask out some parts of the vocabulary and thus created a mask with <code class="language-plaintext highlighter-rouge">-inf</code> values.
For convenience, I also masked out all the irrelevant tokens with <code class="language-plaintext highlighter-rouge">-inf</code> that are not included in the ground truth and also set their relevant class to -1 and set <code class="language-plaintext highlighter-rouge">ignore_index = -1</code> accordingly.
Here’s a sample mask:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">tensor([[0., 0., -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf]])
</span></code></pre></div></div>

<p>I started training my model and the loss was a reasonable float value on the first step, but became <code class="language-plaintext highlighter-rouge">nan</code> in the second step.
This prompted my deep dive.
The TLDR here is that in the example mask above, you can mask the first two columns with <code class="language-plaintext highlighter-rouge">-inf</code> as that’s what you want, but from the third column onwards, mask with 0.</p>

<h2 id="a-simple-example">A Simple Example</h2>

<p>We start with a basic example.
We first create an input tensor and a target tensor.
We enable gradients for the input tensor and set the <code class="language-plaintext highlighter-rouge">ignore_index</code> to -1 as an argument for the <code class="language-plaintext highlighter-rouge">CrossEntropyLoss</code>.
After computing the loss, we perform a backward pass and print the gradients:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># Define the input tensor and target tensor
</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">input_tensor</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">target_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># Assuming ignore_index is -1
</span>
<span class="c1"># Create the CrossEntropyLoss criterion with ignore_index
</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the loss
</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="c1"># Print the loss
</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"Loss:"</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

<span class="c1"># Perform the backward pass
</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Print the gradients
</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"Gradients:"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Output:
Loss: 1.6094379425048828
Gradients:
tensor([[0.1000, -0.4000,  0.1000,  0.1000,  0.1000],
        [ 0.1000,  0.1000, -0.4000,  0.1000,  0.1000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])
</span></code></pre></div></div>

<p>Observe here that the gradients are indeed 0 for the third row, as the <code class="language-plaintext highlighter-rouge">target_tensor</code> has <code class="language-plaintext highlighter-rouge">-1</code> as the third element (or second if 0-indexing).
Naturally, if the gradient is zero, then you don’t do any update on the weights, so this is what we want.</p>

<h2 id="what-happens-when-there-are--infs">What happens when there are <code class="language-plaintext highlighter-rouge">-infs</code>?</h2>

<p>Using the same example above, we now explore what happens when we set an entire row to <code class="language-plaintext highlighter-rouge">-inf</code>.
Specifically, this line was added <code class="language-plaintext highlighter-rouge">input_tensor[-1] = float('-inf')</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># Define the input tensor and target tensor
</span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">input_tensor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span> <span class="c1"># Note this is added!
</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">target_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Assuming ignore_index is -1
</span>
<span class="c1"># Create the CrossEntropyLoss criterion with ignore_index
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="c1"># Print the loss
</span><span class="nf">print</span><span class="p">(</span><span class="s">"Loss:"</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

<span class="c1"># Perform the backward pass
</span><span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Print the gradients
</span><span class="nf">print</span><span class="p">(</span><span class="s">"Gradients:"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Output:
Loss: 1.6094379425048828
Gradients:
tensor([[0.1000, -0.4000,  0.1000,  0.1000,  0.1000],
        [ 0.1000,  0.1000, -0.4000,  0.1000,  0.1000],
        [    nan,     nan,     nan,     nan,     nan]])
</span></code></pre></div></div>

<p>Observe now that the entire third row contains <code class="language-plaintext highlighter-rouge">nan</code> values, which is the cause of our entire problem.</p>

<h2 id="how-can-i-debug-this-in-reality">How can I debug this in reality?</h2>

<p>I read the PyTorch forums and the best way to debug <code class="language-plaintext highlighter-rouge">nan</code> values in loss is perhaps to start by adding <code class="language-plaintext highlighter-rouge">torch.autograd.anomaly_mode.set_detect_anomaly(True)</code> at the top of your script.
After adding that, I immediately saw that the error was:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
</span></code></pre></div></div>

<p>which suggested that I had an issue with my gradients.
That led me to my deep dive and a better understanding of how <code class="language-plaintext highlighter-rouge">ignore_index</code> is used and this blog post.</p>]]></content><author><name></name></author><category term="code" /><summary type="html"><![CDATA[A deep dive into ignore index and how it affects the gradients.]]></summary></entry></feed>