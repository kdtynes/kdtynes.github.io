<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://jkschin.com/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jkschin.com/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-01T12:05:27-04:00</updated><id>https://jkschin.com/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
</subtitle><entry><title type="html">Intricacies of nn.CrossEntropyLoss Ignore Index and Gradients</title><link href="https://jkschin.com/blog/2023/cross-entropy-loss-ignore-index/" rel="alternate" type="text/html" title="Intricacies of nn.CrossEntropyLoss Ignore Index and Gradients" /><published>2023-05-25T11:59:00-04:00</published><updated>2023-05-25T11:59:00-04:00</updated><id>https://jkschin.com/blog/2023/cross-entropy-loss-ignore-index</id><content type="html" xml:base="https://jkschin.com/blog/2023/cross-entropy-loss-ignore-index/"><![CDATA[<h2 id="introduction">Introduction</h2>

<p>In machine learning, PyTorch’s <code class="language-plaintext highlighter-rouge">nn.CrossEntropyLoss</code> is an often-utilized function.
It combines <code class="language-plaintext highlighter-rouge">nn.LogSoftmax()</code> and <code class="language-plaintext highlighter-rouge">nn.NLLLoss()</code> in one single class to compute the cross-entropy loss between the predicted and actual labels.
The CrossEntropyLoss function offers an optional parameter called <code class="language-plaintext highlighter-rouge">ignore_index</code> that can be used to ignore the loss contribution from some specific classes and is widely used in NLP, where padding of a sequence is necessary.
This post delves into the intricacies of this mechanism and how it affects gradients in backpropagation.
I was using <code class="language-plaintext highlighter-rouge">ignore_index</code> and ran into an issue with <code class="language-plaintext highlighter-rouge">nan</code> values after the first optimization step and had to dive deeper into the issue.
As with all code debugging nowadays (and writing), I used ChatGPT to assist me in writing this post.</p>

<h2 id="context">Context</h2>

<p>I was training a GPT based model and the final output is postprocessed with a mask.
In a classic GPT language model, you simply softmax over the vocabulary and it outputs the relevant token.
However, I wanted to mask out some parts of the vocabulary and thus created a mask with <code class="language-plaintext highlighter-rouge">-inf</code> values.
For convenience, I also masked out all the irrelevant tokens with <code class="language-plaintext highlighter-rouge">-inf</code> that are not included in the ground truth and also set their relevant class to -1 and set <code class="language-plaintext highlighter-rouge">ignore_index = -1</code> accordingly.
Here’s a sample mask:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">tensor([[0., 0., -inf, -inf, -inf, -inf],
        [0., 0., -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf],
        [-inf, -inf, -inf, -inf, -inf, -inf]])
</span></code></pre></div></div>

<p>I started training my model and the loss was a reasonable float value on the first step, but became <code class="language-plaintext highlighter-rouge">nan</code> in the second step.
This prompted my deep dive.
The TLDR here is that in the example mask above, you can mask the first two columns with <code class="language-plaintext highlighter-rouge">-inf</code> as that’s what you want, but from the third column onwards, mask with 0.</p>

<h2 id="a-simple-example">A Simple Example</h2>

<p>We start with a basic example.
We first create an input tensor and a target tensor.
We enable gradients for the input tensor and set the <code class="language-plaintext highlighter-rouge">ignore_index</code> to -1 as an argument for the <code class="language-plaintext highlighter-rouge">CrossEntropyLoss</code>.
After computing the loss, we perform a backward pass and print the gradients:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># Define the input tensor and target tensor
</span>
<span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">input_tensor</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">target_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1"># Assuming ignore_index is -1
</span>
<span class="c1"># Create the CrossEntropyLoss criterion with ignore_index
</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the loss
</span>
<span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="c1"># Print the loss
</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"Loss:"</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

<span class="c1"># Perform the backward pass
</span>
<span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Print the gradients
</span>
<span class="nf">print</span><span class="p">(</span><span class="s">"Gradients:"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Output:
Loss: 1.6094379425048828
Gradients:
tensor([[0.1000, -0.4000,  0.1000,  0.1000,  0.1000],
        [ 0.1000,  0.1000, -0.4000,  0.1000,  0.1000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])
</span></code></pre></div></div>

<p>Observe here that the gradients are indeed 0 for the third row, as the <code class="language-plaintext highlighter-rouge">target_tensor</code> has <code class="language-plaintext highlighter-rouge">-1</code> as the third element (or second if 0-indexing).
Naturally, if the gradient is zero, then you don’t do any update on the weights, so this is what we want.</p>

<h2 id="what-happens-when-there-are--infs">What happens when there are <code class="language-plaintext highlighter-rouge">-infs</code>?</h2>

<p>Using the same example above, we now explore what happens when we set an entire row to <code class="language-plaintext highlighter-rouge">-inf</code>.
Specifically, this line was added <code class="language-plaintext highlighter-rouge">input_tensor[-1] = float('-inf')</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">torch</span>
<span class="kn">import</span> <span class="n">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>

<span class="c1"># Define the input tensor and target tensor
</span><span class="n">input_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">ones</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
<span class="n">input_tensor</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nf">float</span><span class="p">(</span><span class="s">'-inf'</span><span class="p">)</span> <span class="c1"># Note this is added!
</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">requires_grad</span> <span class="o">=</span> <span class="bp">True</span>
<span class="n">target_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># Assuming ignore_index is -1
</span>
<span class="c1"># Create the CrossEntropyLoss criterion with ignore_index
</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">ignore_index</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Compute the loss
</span><span class="n">loss</span> <span class="o">=</span> <span class="nf">criterion</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">target_tensor</span><span class="p">)</span>

<span class="c1"># Print the loss
</span><span class="nf">print</span><span class="p">(</span><span class="s">"Loss:"</span><span class="p">,</span> <span class="n">loss</span><span class="p">.</span><span class="nf">item</span><span class="p">())</span>

<span class="c1"># Perform the backward pass
</span><span class="n">loss</span><span class="p">.</span><span class="nf">backward</span><span class="p">()</span>

<span class="c1"># Print the gradients
</span><span class="nf">print</span><span class="p">(</span><span class="s">"Gradients:"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">.</span><span class="n">grad</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">Output:
Loss: 1.6094379425048828
Gradients:
tensor([[0.1000, -0.4000,  0.1000,  0.1000,  0.1000],
        [ 0.1000,  0.1000, -0.4000,  0.1000,  0.1000],
        [    nan,     nan,     nan,     nan,     nan]])
</span></code></pre></div></div>

<p>Observe now that the entire third row contains <code class="language-plaintext highlighter-rouge">nan</code> values, which is the cause of our entire problem.</p>

<h2 id="how-can-i-debug-this-in-reality">How can I debug this in reality?</h2>

<p>I read the PyTorch forums and the best way to debug <code class="language-plaintext highlighter-rouge">nan</code> values in loss is perhaps to start by adding <code class="language-plaintext highlighter-rouge">torch.autograd.anomaly_mode.set_detect_anomaly(True)</code> at the top of your script.
After adding that, I immediately saw that the error was:</p>

<div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="go">RuntimeError: Function 'LogSoftmaxBackward0' returned nan values in its 0th output.
</span></code></pre></div></div>

<p>which suggested that I had an issue with my gradients.
That led me to my deep dive and a better understanding of how <code class="language-plaintext highlighter-rouge">ignore_index</code> is used and this blog post.</p>]]></content><author><name></name></author><category term="code" /><summary type="html"><![CDATA[A deep dive into ignore index and how it affects the gradients.]]></summary></entry></feed>